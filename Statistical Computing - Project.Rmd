---
title: "Statistical Computing - Project"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

#### Installing required packages
```{r}
### Install required packages
library(ggplot2)
library(dplyr)
library(corrgram)
library(GGally)
library(grid)
library(gridExtra)
library(broom)
```

#### Importing the data and sanity checks
```{r}
# Importing the dataset
insurance <- read.csv("insurance.csv")
head(insurance)
```

```{r}
# Check dataset dimensions
dim(insurance)

# Check for null values
sum(is.na(insurance))
```
```{r}
# Checking datatype of all columns
sapply(insurance,class)
```
```{r}
# Converting sex and smoker columns to factor for modelling purpose
col <- c('sex' ,'smoker')
insurance[,col] <- lapply(insurance[,col] , factor)
str(insurance)
```
#### Splitting the data into train-test dataset (70-30)

```{r}
# Splitting data into 70-30 train-test data
train_id <- sample(nrow(insurance), .70*nrow(insurance))

insurance_train <- insurance[train_id,]
insurance_test <- insurance[-train_id,]

nrow(insurance)
nrow(insurance_train)/nrow(insurance)*100
nrow(insurance_test)/nrow(insurance)*100
```
#### Fitting Linear Regression model on the train dataset
```{r}
# Linear Regression model on train data with all predictor variables
fit1 <- lm(charges ~ age + sex + bmi + children + smoker, data=insurance_train)
summary(fit1)
```
```{r}
# Linear Regression model on train data without the parameter sex
fit2 <- lm(charges ~ age + bmi + children + smoker, data=insurance_train)
summary(fit2)
```
```{r}
# Using partial f-test to check relevance of both models wrt. each other
anova(fit2, fit1)
```
The p-value is > 0.05, hence the additional variable (sex) in fit2 does not explain the data better. Hence, we will go ahead with the simpler model with the following variables: charges ~ age + bmi + children + smoker

```{r}
# Calculating predicted values for test data
fit2_preds <- predict(fit2, insurance_test)
str(fit2_preds)
```
```{r}
# Store residual values
res_2 <- fit2$residuals
str(res_2)
```
#### Residual diagnostics
```{r figures side, fig.show="hold", out.width="50%"}
ggplot(data.frame(res_2), aes(res_2)) + geom_histogram()

```
The data does not seem to be normally distributed. From visual observations, the data seems to be right skewed. The mean looks approximately centered at zero, but this can also be verified using t.test

```{r}
# Checking for assumption of zero mean using t.test()
t.test(res_2)
```
The p-value is reported as 1 for both models. Thus, there is no evidence to reject the assumption that the mean of residuals is zero.

```{r}
# Creating dataframe for predictions and residuals using augment()

fit2_df <- fit2 %>% 
	augment() %>%
	mutate(row_num = 1:n())
head(fit2_df)
```

```{r}
# Q-Q plot
ggplot(fit2_df, aes(sample = .std.resid)) +
	geom_qq(alpha = 0.6, size = 1.5) +
	geom_qq_line(linetype = "dashed", color = "red2") +
	xlab("Theoretical quantile") +
	ylab("Sample quantile") +
	theme_grey()

```

```{r}
# Fitted vs Residual
ggplot(fit2, aes(x = .fitted, y =.resid)) +
	geom_point(alpha = 0.6, size = 1.5) +
	geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
	geom_hline(yintercept = c(-2, 2), linetype = "dotted") +
	xlab("Fitted value") +
	ylab(" residual") +
	theme_grey()

```

From the above plots, we can infer that assumption of normality and constant variance has been violated.

```{r}
# Checking for multi-collinearity
car::vif(fit2)
```
Since the VIF (Variance Inflation Factor) is < 10 for all parameters, the assumption of multi-collinearity  has been met.

#### Applying box cox transformation
```{r}
# Find optimal lambda value via ML estimation
bc <- MASS::boxcox(charges ~ age + bmi + children + smoker, data=insurance_train)
lambda <- bc$x[which.max(bc$y)]
print(lambda)

# boxcox transformed model
insurance_train$charges2 <- (insurance_train$charges ^ lambda - 1) / lambda
fit2_bc <- lm(charges2 ~ age + bmi + children + smoker, data=insurance_train)
summary(fit2_bc)
```
```{r}
# Store residual values
res_2b <- fit2_bc$residuals
str(res_2b)
```

```{r, figures-side, fig.show="hold", out.width="50%"}
par(mfrow = c(2,4))
ggplot(data.frame(res_2b), aes(res_2b)) + geom_histogram()

```
```{r}
# Creating dataframe for predictions and residuals using augment()

fit2b_df <- fit2_bc %>% 
	augment() %>%
	mutate(row_num = 1:n())
head(fit2b_df)
```
```{r}
# Q-Q plot
ggplot(fit2b_df, aes(sample = .std.resid)) +
	geom_qq(alpha = 0.6, size = 1.5) +
	geom_qq_line(linetype = "dashed", color = "red2") +
	xlab("Theoretical quantile") +
	ylab("Sample quantile") +
	theme_grey()

```

```{r}
# Fitted vs Residual
ggplot(fit2_bc, aes(x = .fitted, y =.resid)) +
	geom_point(alpha = 0.6, size = 1.5) +
	geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
	geom_hline(yintercept = c(-2, 2), linetype = "dotted") +
	xlab("Fitted value") +
	ylab(" residual") +
	theme_grey()

```
The r-squared value has improved slightly on applying box-cox transformation with a lambda value of 0.1818, but the assumption of normality is still being violated. 

#### Transforming test data
```{r}
insurance_test$charges2 <- (insurance_test$charges ^ lambda - 1) / lambda
```

```{r}
# Calculating predicted values for test data
fit2bc_preds <- predict(fit2_bc, insurance_test)
str(fit2bc_preds)
```

#### Calculating model performance metrics
```{r}
calc_performance <- function(actual, pred) {
  
  rmse <- sqrt(mean((actual - pred)**2))
  mae <- mean(abs(actual - pred))
  mape <- mean(abs((actual-pred)/actual))
  
  retvals <- list(rmse = rmse, mae = mae, mape = mape)
  return(retvals)
}
```

```{r}
# Model performance metrics for the model without transformation

metrics_1 <- calc_performance(insurance_test$charges,fit2_preds)
metrics_1
```

```{r}
# Model performance metrics for transformed model

metrics_2 <- calc_performance(insurance_test$charges2,fit2bc_preds)
metrics_2

```

RMSE values for both the models cannot be compared since the one of them is a transformed one.
On comparing MAPE (Mean absolute percentage error), we can conclude that the transformed model is significantly better. 



